{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 4500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006670557825398149,
      "grad_norm": 0.27139148116111755,
      "learning_rate": 0.0001996,
      "loss": 2.9975,
      "step": 10
    },
    {
      "epoch": 0.013341115650796298,
      "grad_norm": 0.33632513880729675,
      "learning_rate": 0.00019915555555555557,
      "loss": 2.9065,
      "step": 20
    },
    {
      "epoch": 0.020011673476194446,
      "grad_norm": 0.3807283937931061,
      "learning_rate": 0.0001987111111111111,
      "loss": 2.7167,
      "step": 30
    },
    {
      "epoch": 0.026682231301592595,
      "grad_norm": 0.4145461320877075,
      "learning_rate": 0.00019826666666666667,
      "loss": 2.6633,
      "step": 40
    },
    {
      "epoch": 0.033352789126990745,
      "grad_norm": 0.4095216989517212,
      "learning_rate": 0.00019782222222222224,
      "loss": 2.569,
      "step": 50
    },
    {
      "epoch": 0.04002334695238889,
      "grad_norm": 0.46652141213417053,
      "learning_rate": 0.0001973777777777778,
      "loss": 2.6582,
      "step": 60
    },
    {
      "epoch": 0.046693904777787044,
      "grad_norm": 0.3681914210319519,
      "learning_rate": 0.00019693333333333334,
      "loss": 2.5668,
      "step": 70
    },
    {
      "epoch": 0.05336446260318519,
      "grad_norm": 0.5172369480133057,
      "learning_rate": 0.0001964888888888889,
      "loss": 2.5664,
      "step": 80
    },
    {
      "epoch": 0.06003502042858334,
      "grad_norm": 0.7611081004142761,
      "learning_rate": 0.00019604444444444445,
      "loss": 2.5149,
      "step": 90
    },
    {
      "epoch": 0.06670557825398149,
      "grad_norm": 0.6038681268692017,
      "learning_rate": 0.0001956,
      "loss": 2.4051,
      "step": 100
    },
    {
      "epoch": 0.07337613607937964,
      "grad_norm": 0.5517551898956299,
      "learning_rate": 0.00019515555555555555,
      "loss": 2.2984,
      "step": 110
    },
    {
      "epoch": 0.08004669390477778,
      "grad_norm": 0.4905649423599243,
      "learning_rate": 0.00019471111111111112,
      "loss": 2.384,
      "step": 120
    },
    {
      "epoch": 0.08671725173017594,
      "grad_norm": 0.600707471370697,
      "learning_rate": 0.00019426666666666668,
      "loss": 2.2894,
      "step": 130
    },
    {
      "epoch": 0.09338780955557409,
      "grad_norm": 0.6324772834777832,
      "learning_rate": 0.00019382222222222225,
      "loss": 2.4451,
      "step": 140
    },
    {
      "epoch": 0.10005836738097224,
      "grad_norm": 0.6151325106620789,
      "learning_rate": 0.00019337777777777779,
      "loss": 2.3654,
      "step": 150
    },
    {
      "epoch": 0.10672892520637038,
      "grad_norm": 0.743557333946228,
      "learning_rate": 0.00019293333333333335,
      "loss": 2.2832,
      "step": 160
    },
    {
      "epoch": 0.11339948303176853,
      "grad_norm": 0.6187404990196228,
      "learning_rate": 0.0001924888888888889,
      "loss": 2.4165,
      "step": 170
    },
    {
      "epoch": 0.12007004085716667,
      "grad_norm": 0.642687201499939,
      "learning_rate": 0.00019204444444444446,
      "loss": 2.302,
      "step": 180
    },
    {
      "epoch": 0.12674059868256482,
      "grad_norm": 0.653734028339386,
      "learning_rate": 0.0001916,
      "loss": 2.3952,
      "step": 190
    },
    {
      "epoch": 0.13341115650796298,
      "grad_norm": 0.5184223651885986,
      "learning_rate": 0.00019115555555555556,
      "loss": 2.2595,
      "step": 200
    },
    {
      "epoch": 0.14008171433336114,
      "grad_norm": 0.5833678841590881,
      "learning_rate": 0.00019071111111111113,
      "loss": 2.3014,
      "step": 210
    },
    {
      "epoch": 0.14675227215875927,
      "grad_norm": 0.6271242499351501,
      "learning_rate": 0.0001902666666666667,
      "loss": 2.3556,
      "step": 220
    },
    {
      "epoch": 0.15342282998415743,
      "grad_norm": 0.6543484926223755,
      "learning_rate": 0.00018982222222222223,
      "loss": 2.3031,
      "step": 230
    },
    {
      "epoch": 0.16009338780955557,
      "grad_norm": 0.741921603679657,
      "learning_rate": 0.0001893777777777778,
      "loss": 2.2202,
      "step": 240
    },
    {
      "epoch": 0.16676394563495373,
      "grad_norm": 0.5288332104682922,
      "learning_rate": 0.00018893333333333334,
      "loss": 2.2873,
      "step": 250
    },
    {
      "epoch": 0.17343450346035189,
      "grad_norm": 0.6301289200782776,
      "learning_rate": 0.0001884888888888889,
      "loss": 2.2497,
      "step": 260
    },
    {
      "epoch": 0.18010506128575002,
      "grad_norm": 0.5750779509544373,
      "learning_rate": 0.00018804444444444444,
      "loss": 2.219,
      "step": 270
    },
    {
      "epoch": 0.18677561911114818,
      "grad_norm": 0.6981350779533386,
      "learning_rate": 0.0001876,
      "loss": 2.3238,
      "step": 280
    },
    {
      "epoch": 0.1934461769365463,
      "grad_norm": 0.6250690221786499,
      "learning_rate": 0.00018715555555555557,
      "loss": 2.2101,
      "step": 290
    },
    {
      "epoch": 0.20011673476194447,
      "grad_norm": 0.75470370054245,
      "learning_rate": 0.00018671111111111114,
      "loss": 2.2605,
      "step": 300
    },
    {
      "epoch": 0.2067872925873426,
      "grad_norm": 0.639401376247406,
      "learning_rate": 0.00018626666666666668,
      "loss": 2.273,
      "step": 310
    },
    {
      "epoch": 0.21345785041274076,
      "grad_norm": 0.52271968126297,
      "learning_rate": 0.00018582222222222224,
      "loss": 2.3207,
      "step": 320
    },
    {
      "epoch": 0.22012840823813892,
      "grad_norm": 0.7425433397293091,
      "learning_rate": 0.00018537777777777778,
      "loss": 2.2891,
      "step": 330
    },
    {
      "epoch": 0.22679896606353706,
      "grad_norm": 0.6847482919692993,
      "learning_rate": 0.00018493333333333335,
      "loss": 2.2588,
      "step": 340
    },
    {
      "epoch": 0.23346952388893522,
      "grad_norm": 0.5448423624038696,
      "learning_rate": 0.00018448888888888889,
      "loss": 2.1708,
      "step": 350
    },
    {
      "epoch": 0.24014008171433335,
      "grad_norm": 0.7425100207328796,
      "learning_rate": 0.00018404444444444445,
      "loss": 2.1815,
      "step": 360
    },
    {
      "epoch": 0.2468106395397315,
      "grad_norm": 0.8830766677856445,
      "learning_rate": 0.00018360000000000002,
      "loss": 2.1924,
      "step": 370
    },
    {
      "epoch": 0.25348119736512964,
      "grad_norm": 0.6082636713981628,
      "learning_rate": 0.00018315555555555556,
      "loss": 2.2388,
      "step": 380
    },
    {
      "epoch": 0.2601517551905278,
      "grad_norm": 0.8474748730659485,
      "learning_rate": 0.00018271111111111112,
      "loss": 2.1184,
      "step": 390
    },
    {
      "epoch": 0.26682231301592596,
      "grad_norm": 0.728401780128479,
      "learning_rate": 0.0001822666666666667,
      "loss": 2.1127,
      "step": 400
    },
    {
      "epoch": 0.2734928708413241,
      "grad_norm": 0.8550076484680176,
      "learning_rate": 0.00018182222222222223,
      "loss": 2.3105,
      "step": 410
    },
    {
      "epoch": 0.2801634286667223,
      "grad_norm": 0.6876714825630188,
      "learning_rate": 0.0001813777777777778,
      "loss": 2.2366,
      "step": 420
    },
    {
      "epoch": 0.2868339864921204,
      "grad_norm": 0.5940482020378113,
      "learning_rate": 0.00018093333333333333,
      "loss": 2.1998,
      "step": 430
    },
    {
      "epoch": 0.29350454431751855,
      "grad_norm": 0.5310220718383789,
      "learning_rate": 0.0001804888888888889,
      "loss": 2.1776,
      "step": 440
    },
    {
      "epoch": 0.3001751021429167,
      "grad_norm": 0.6822366118431091,
      "learning_rate": 0.00018004444444444446,
      "loss": 2.1755,
      "step": 450
    },
    {
      "epoch": 0.30684565996831487,
      "grad_norm": 0.7014957070350647,
      "learning_rate": 0.0001796,
      "loss": 2.1884,
      "step": 460
    },
    {
      "epoch": 0.313516217793713,
      "grad_norm": 0.590572714805603,
      "learning_rate": 0.00017915555555555557,
      "loss": 2.216,
      "step": 470
    },
    {
      "epoch": 0.32018677561911113,
      "grad_norm": 0.7103872895240784,
      "learning_rate": 0.0001787111111111111,
      "loss": 2.1844,
      "step": 480
    },
    {
      "epoch": 0.3268573334445093,
      "grad_norm": 0.6868285536766052,
      "learning_rate": 0.00017826666666666667,
      "loss": 2.3225,
      "step": 490
    },
    {
      "epoch": 0.33352789126990745,
      "grad_norm": 0.6137939095497131,
      "learning_rate": 0.0001778222222222222,
      "loss": 2.2327,
      "step": 500
    },
    {
      "epoch": 0.3401984490953056,
      "grad_norm": 0.6813240647315979,
      "learning_rate": 0.00017737777777777778,
      "loss": 2.1085,
      "step": 510
    },
    {
      "epoch": 0.34686900692070377,
      "grad_norm": 0.6390339732170105,
      "learning_rate": 0.00017693333333333334,
      "loss": 2.1367,
      "step": 520
    },
    {
      "epoch": 0.3535395647461019,
      "grad_norm": 0.7456470131874084,
      "learning_rate": 0.0001764888888888889,
      "loss": 2.1588,
      "step": 530
    },
    {
      "epoch": 0.36021012257150004,
      "grad_norm": 0.7861881256103516,
      "learning_rate": 0.00017604444444444445,
      "loss": 2.1714,
      "step": 540
    },
    {
      "epoch": 0.3668806803968982,
      "grad_norm": 0.6360123157501221,
      "learning_rate": 0.0001756,
      "loss": 2.1633,
      "step": 550
    },
    {
      "epoch": 0.37355123822229636,
      "grad_norm": 0.7354249358177185,
      "learning_rate": 0.00017515555555555555,
      "loss": 2.153,
      "step": 560
    },
    {
      "epoch": 0.3802217960476945,
      "grad_norm": 0.9378804564476013,
      "learning_rate": 0.00017471111111111112,
      "loss": 2.2275,
      "step": 570
    },
    {
      "epoch": 0.3868923538730926,
      "grad_norm": 0.8426141738891602,
      "learning_rate": 0.00017426666666666666,
      "loss": 2.1275,
      "step": 580
    },
    {
      "epoch": 0.3935629116984908,
      "grad_norm": 0.7811610102653503,
      "learning_rate": 0.00017382222222222222,
      "loss": 2.1219,
      "step": 590
    },
    {
      "epoch": 0.40023346952388894,
      "grad_norm": 0.6912941932678223,
      "learning_rate": 0.0001733777777777778,
      "loss": 2.0158,
      "step": 600
    },
    {
      "epoch": 0.4069040273492871,
      "grad_norm": 0.7869628667831421,
      "learning_rate": 0.00017293333333333335,
      "loss": 2.1944,
      "step": 610
    },
    {
      "epoch": 0.4135745851746852,
      "grad_norm": 0.691901683807373,
      "learning_rate": 0.0001724888888888889,
      "loss": 2.1753,
      "step": 620
    },
    {
      "epoch": 0.42024514300008337,
      "grad_norm": 0.7489545941352844,
      "learning_rate": 0.00017204444444444446,
      "loss": 2.1558,
      "step": 630
    },
    {
      "epoch": 0.4269157008254815,
      "grad_norm": 0.7553005218505859,
      "learning_rate": 0.0001716,
      "loss": 2.1476,
      "step": 640
    },
    {
      "epoch": 0.4335862586508797,
      "grad_norm": 1.0210269689559937,
      "learning_rate": 0.00017115555555555556,
      "loss": 2.2326,
      "step": 650
    },
    {
      "epoch": 0.44025681647627785,
      "grad_norm": 0.681294322013855,
      "learning_rate": 0.0001707111111111111,
      "loss": 2.0342,
      "step": 660
    },
    {
      "epoch": 0.44692737430167595,
      "grad_norm": 0.9360763430595398,
      "learning_rate": 0.0001702666666666667,
      "loss": 2.1273,
      "step": 670
    },
    {
      "epoch": 0.4535979321270741,
      "grad_norm": 0.8749818205833435,
      "learning_rate": 0.00016982222222222223,
      "loss": 2.1128,
      "step": 680
    },
    {
      "epoch": 0.46026848995247227,
      "grad_norm": 0.6499338746070862,
      "learning_rate": 0.0001693777777777778,
      "loss": 2.1221,
      "step": 690
    },
    {
      "epoch": 0.46693904777787043,
      "grad_norm": 0.6425748467445374,
      "learning_rate": 0.00016893333333333334,
      "loss": 2.133,
      "step": 700
    },
    {
      "epoch": 0.4736096056032686,
      "grad_norm": 0.8100475668907166,
      "learning_rate": 0.0001684888888888889,
      "loss": 2.2207,
      "step": 710
    },
    {
      "epoch": 0.4802801634286667,
      "grad_norm": 0.7310921549797058,
      "learning_rate": 0.00016804444444444444,
      "loss": 2.1857,
      "step": 720
    },
    {
      "epoch": 0.48695072125406486,
      "grad_norm": 0.8034652471542358,
      "learning_rate": 0.0001676,
      "loss": 2.1007,
      "step": 730
    },
    {
      "epoch": 0.493621279079463,
      "grad_norm": 0.8256924152374268,
      "learning_rate": 0.00016715555555555555,
      "loss": 2.1835,
      "step": 740
    },
    {
      "epoch": 0.5002918369048611,
      "grad_norm": 0.5928146839141846,
      "learning_rate": 0.00016671111111111114,
      "loss": 2.0529,
      "step": 750
    },
    {
      "epoch": 0.5069623947302593,
      "grad_norm": 0.7071451544761658,
      "learning_rate": 0.00016626666666666668,
      "loss": 2.123,
      "step": 760
    },
    {
      "epoch": 0.5136329525556574,
      "grad_norm": 0.6202550530433655,
      "learning_rate": 0.00016582222222222224,
      "loss": 2.1439,
      "step": 770
    },
    {
      "epoch": 0.5203035103810556,
      "grad_norm": 0.7574056386947632,
      "learning_rate": 0.00016537777777777778,
      "loss": 2.1568,
      "step": 780
    },
    {
      "epoch": 0.5269740682064538,
      "grad_norm": 0.8663021922111511,
      "learning_rate": 0.00016493333333333335,
      "loss": 2.0521,
      "step": 790
    },
    {
      "epoch": 0.5336446260318519,
      "grad_norm": 0.7971614003181458,
      "learning_rate": 0.0001644888888888889,
      "loss": 2.0751,
      "step": 800
    },
    {
      "epoch": 0.5403151838572501,
      "grad_norm": 0.9397456645965576,
      "learning_rate": 0.00016404444444444445,
      "loss": 2.0843,
      "step": 810
    },
    {
      "epoch": 0.5469857416826482,
      "grad_norm": 0.7916479706764221,
      "learning_rate": 0.0001636,
      "loss": 2.1421,
      "step": 820
    },
    {
      "epoch": 0.5536562995080464,
      "grad_norm": 0.6466131806373596,
      "learning_rate": 0.00016315555555555559,
      "loss": 2.1688,
      "step": 830
    },
    {
      "epoch": 0.5603268573334446,
      "grad_norm": 0.5968828797340393,
      "learning_rate": 0.00016271111111111112,
      "loss": 2.1906,
      "step": 840
    },
    {
      "epoch": 0.5669974151588426,
      "grad_norm": 0.8301454782485962,
      "learning_rate": 0.0001622666666666667,
      "loss": 2.1577,
      "step": 850
    },
    {
      "epoch": 0.5736679729842408,
      "grad_norm": 0.6258218288421631,
      "learning_rate": 0.00016182222222222223,
      "loss": 2.146,
      "step": 860
    },
    {
      "epoch": 0.5803385308096389,
      "grad_norm": 0.784542977809906,
      "learning_rate": 0.0001613777777777778,
      "loss": 2.2399,
      "step": 870
    },
    {
      "epoch": 0.5870090886350371,
      "grad_norm": 0.8963592648506165,
      "learning_rate": 0.00016093333333333333,
      "loss": 2.1324,
      "step": 880
    },
    {
      "epoch": 0.5936796464604353,
      "grad_norm": 0.6417927145957947,
      "learning_rate": 0.0001604888888888889,
      "loss": 2.1168,
      "step": 890
    },
    {
      "epoch": 0.6003502042858334,
      "grad_norm": 0.8494239449501038,
      "learning_rate": 0.00016004444444444444,
      "loss": 2.1671,
      "step": 900
    },
    {
      "epoch": 0.6070207621112316,
      "grad_norm": 0.8711056709289551,
      "learning_rate": 0.0001596,
      "loss": 2.1269,
      "step": 910
    },
    {
      "epoch": 0.6136913199366297,
      "grad_norm": 0.6624898314476013,
      "learning_rate": 0.00015915555555555557,
      "loss": 2.1004,
      "step": 920
    },
    {
      "epoch": 0.6203618777620279,
      "grad_norm": 0.787954568862915,
      "learning_rate": 0.00015871111111111114,
      "loss": 2.1223,
      "step": 930
    },
    {
      "epoch": 0.627032435587426,
      "grad_norm": 0.9642940163612366,
      "learning_rate": 0.00015826666666666667,
      "loss": 2.1178,
      "step": 940
    },
    {
      "epoch": 0.6337029934128241,
      "grad_norm": 0.683406412601471,
      "learning_rate": 0.00015782222222222224,
      "loss": 2.2052,
      "step": 950
    },
    {
      "epoch": 0.6403735512382223,
      "grad_norm": 0.677297830581665,
      "learning_rate": 0.00015737777777777778,
      "loss": 2.2443,
      "step": 960
    },
    {
      "epoch": 0.6470441090636204,
      "grad_norm": 0.6900399327278137,
      "learning_rate": 0.00015693333333333334,
      "loss": 2.1772,
      "step": 970
    },
    {
      "epoch": 0.6537146668890186,
      "grad_norm": 0.6865572333335876,
      "learning_rate": 0.0001564888888888889,
      "loss": 2.1317,
      "step": 980
    },
    {
      "epoch": 0.6603852247144167,
      "grad_norm": 0.7297844886779785,
      "learning_rate": 0.00015604444444444445,
      "loss": 2.0638,
      "step": 990
    },
    {
      "epoch": 0.6670557825398149,
      "grad_norm": 0.8759883046150208,
      "learning_rate": 0.00015560000000000001,
      "loss": 2.0802,
      "step": 1000
    },
    {
      "epoch": 0.6737263403652131,
      "grad_norm": 0.7069225311279297,
      "learning_rate": 0.00015515555555555555,
      "loss": 2.1508,
      "step": 1010
    },
    {
      "epoch": 0.6803968981906112,
      "grad_norm": 0.6179034113883972,
      "learning_rate": 0.00015471111111111112,
      "loss": 2.0322,
      "step": 1020
    },
    {
      "epoch": 0.6870674560160094,
      "grad_norm": 0.8815830945968628,
      "learning_rate": 0.00015426666666666666,
      "loss": 2.0842,
      "step": 1030
    },
    {
      "epoch": 0.6937380138414075,
      "grad_norm": 0.6872398257255554,
      "learning_rate": 0.00015382222222222222,
      "loss": 2.1639,
      "step": 1040
    },
    {
      "epoch": 0.7004085716668056,
      "grad_norm": 0.6095442771911621,
      "learning_rate": 0.00015337777777777776,
      "loss": 2.0792,
      "step": 1050
    },
    {
      "epoch": 0.7070791294922038,
      "grad_norm": 0.7176697850227356,
      "learning_rate": 0.00015293333333333336,
      "loss": 2.1762,
      "step": 1060
    },
    {
      "epoch": 0.7137496873176019,
      "grad_norm": 0.6683450937271118,
      "learning_rate": 0.0001524888888888889,
      "loss": 2.0693,
      "step": 1070
    },
    {
      "epoch": 0.7204202451430001,
      "grad_norm": 0.8066493272781372,
      "learning_rate": 0.00015204444444444446,
      "loss": 2.0993,
      "step": 1080
    },
    {
      "epoch": 0.7270908029683982,
      "grad_norm": 0.6957029700279236,
      "learning_rate": 0.0001516,
      "loss": 2.0803,
      "step": 1090
    },
    {
      "epoch": 0.7337613607937964,
      "grad_norm": 0.8595916628837585,
      "learning_rate": 0.00015115555555555556,
      "loss": 1.9989,
      "step": 1100
    },
    {
      "epoch": 0.7404319186191946,
      "grad_norm": 1.079329490661621,
      "learning_rate": 0.0001507111111111111,
      "loss": 2.194,
      "step": 1110
    },
    {
      "epoch": 0.7471024764445927,
      "grad_norm": 0.909101665019989,
      "learning_rate": 0.00015026666666666667,
      "loss": 2.0985,
      "step": 1120
    },
    {
      "epoch": 0.7537730342699909,
      "grad_norm": 0.9125761389732361,
      "learning_rate": 0.0001498222222222222,
      "loss": 2.0528,
      "step": 1130
    },
    {
      "epoch": 0.760443592095389,
      "grad_norm": 0.7358615398406982,
      "learning_rate": 0.0001493777777777778,
      "loss": 2.134,
      "step": 1140
    },
    {
      "epoch": 0.7671141499207871,
      "grad_norm": 0.669025182723999,
      "learning_rate": 0.00014893333333333334,
      "loss": 2.1503,
      "step": 1150
    },
    {
      "epoch": 0.7737847077461852,
      "grad_norm": 1.0476244688034058,
      "learning_rate": 0.0001484888888888889,
      "loss": 2.0362,
      "step": 1160
    },
    {
      "epoch": 0.7804552655715834,
      "grad_norm": 0.7076849937438965,
      "learning_rate": 0.00014804444444444444,
      "loss": 2.0973,
      "step": 1170
    },
    {
      "epoch": 0.7871258233969816,
      "grad_norm": 0.6298254728317261,
      "learning_rate": 0.0001476,
      "loss": 2.0911,
      "step": 1180
    },
    {
      "epoch": 0.7937963812223797,
      "grad_norm": 0.6594511866569519,
      "learning_rate": 0.00014715555555555555,
      "loss": 1.9556,
      "step": 1190
    },
    {
      "epoch": 0.8004669390477779,
      "grad_norm": 0.9082775712013245,
      "learning_rate": 0.00014671111111111111,
      "loss": 2.0639,
      "step": 1200
    },
    {
      "epoch": 0.807137496873176,
      "grad_norm": 1.308314561843872,
      "learning_rate": 0.00014626666666666665,
      "loss": 2.1285,
      "step": 1210
    },
    {
      "epoch": 0.8138080546985742,
      "grad_norm": 0.863510251045227,
      "learning_rate": 0.00014582222222222225,
      "loss": 2.1921,
      "step": 1220
    },
    {
      "epoch": 0.8204786125239724,
      "grad_norm": 0.7190967202186584,
      "learning_rate": 0.00014537777777777778,
      "loss": 2.0891,
      "step": 1230
    },
    {
      "epoch": 0.8271491703493704,
      "grad_norm": 1.0290544033050537,
      "learning_rate": 0.00014493333333333335,
      "loss": 2.129,
      "step": 1240
    },
    {
      "epoch": 0.8338197281747686,
      "grad_norm": 0.7207701802253723,
      "learning_rate": 0.0001444888888888889,
      "loss": 2.0428,
      "step": 1250
    },
    {
      "epoch": 0.8404902860001667,
      "grad_norm": 0.729571521282196,
      "learning_rate": 0.00014404444444444446,
      "loss": 2.0829,
      "step": 1260
    },
    {
      "epoch": 0.8471608438255649,
      "grad_norm": 0.6394619345664978,
      "learning_rate": 0.0001436,
      "loss": 2.1232,
      "step": 1270
    },
    {
      "epoch": 0.853831401650963,
      "grad_norm": 0.7444118857383728,
      "learning_rate": 0.00014315555555555556,
      "loss": 2.1247,
      "step": 1280
    },
    {
      "epoch": 0.8605019594763612,
      "grad_norm": 0.7776960134506226,
      "learning_rate": 0.00014271111111111113,
      "loss": 2.0556,
      "step": 1290
    },
    {
      "epoch": 0.8671725173017594,
      "grad_norm": 0.7124752998352051,
      "learning_rate": 0.0001422666666666667,
      "loss": 2.1608,
      "step": 1300
    },
    {
      "epoch": 0.8738430751271575,
      "grad_norm": 0.8152880668640137,
      "learning_rate": 0.00014182222222222223,
      "loss": 2.0993,
      "step": 1310
    },
    {
      "epoch": 0.8805136329525557,
      "grad_norm": 0.6218685507774353,
      "learning_rate": 0.0001413777777777778,
      "loss": 2.0852,
      "step": 1320
    },
    {
      "epoch": 0.8871841907779539,
      "grad_norm": 0.6246348023414612,
      "learning_rate": 0.00014093333333333333,
      "loss": 2.1141,
      "step": 1330
    },
    {
      "epoch": 0.8938547486033519,
      "grad_norm": 0.7882947325706482,
      "learning_rate": 0.0001404888888888889,
      "loss": 2.1646,
      "step": 1340
    },
    {
      "epoch": 0.9005253064287501,
      "grad_norm": 0.5988253951072693,
      "learning_rate": 0.00014004444444444444,
      "loss": 2.1401,
      "step": 1350
    },
    {
      "epoch": 0.9071958642541482,
      "grad_norm": 0.704926609992981,
      "learning_rate": 0.0001396,
      "loss": 2.1229,
      "step": 1360
    },
    {
      "epoch": 0.9138664220795464,
      "grad_norm": 1.0246639251708984,
      "learning_rate": 0.00013915555555555557,
      "loss": 2.115,
      "step": 1370
    },
    {
      "epoch": 0.9205369799049445,
      "grad_norm": 0.6813921928405762,
      "learning_rate": 0.00013871111111111114,
      "loss": 2.1228,
      "step": 1380
    },
    {
      "epoch": 0.9272075377303427,
      "grad_norm": 1.0330281257629395,
      "learning_rate": 0.00013826666666666668,
      "loss": 2.0549,
      "step": 1390
    },
    {
      "epoch": 0.9338780955557409,
      "grad_norm": 0.9203210473060608,
      "learning_rate": 0.00013782222222222224,
      "loss": 2.074,
      "step": 1400
    },
    {
      "epoch": 0.940548653381139,
      "grad_norm": 0.8165903091430664,
      "learning_rate": 0.00013737777777777778,
      "loss": 2.1077,
      "step": 1410
    },
    {
      "epoch": 0.9472192112065372,
      "grad_norm": 0.6497462391853333,
      "learning_rate": 0.00013693333333333335,
      "loss": 2.0406,
      "step": 1420
    },
    {
      "epoch": 0.9538897690319353,
      "grad_norm": 0.9707908034324646,
      "learning_rate": 0.00013648888888888888,
      "loss": 2.1259,
      "step": 1430
    },
    {
      "epoch": 0.9605603268573334,
      "grad_norm": 0.632710337638855,
      "learning_rate": 0.00013604444444444445,
      "loss": 1.9812,
      "step": 1440
    },
    {
      "epoch": 0.9672308846827316,
      "grad_norm": 0.7116287350654602,
      "learning_rate": 0.00013560000000000002,
      "loss": 2.0857,
      "step": 1450
    },
    {
      "epoch": 0.9739014425081297,
      "grad_norm": 0.7581008076667786,
      "learning_rate": 0.00013515555555555556,
      "loss": 2.1028,
      "step": 1460
    },
    {
      "epoch": 0.9805720003335279,
      "grad_norm": 0.6536960601806641,
      "learning_rate": 0.00013471111111111112,
      "loss": 2.019,
      "step": 1470
    },
    {
      "epoch": 0.987242558158926,
      "grad_norm": 0.8745313286781311,
      "learning_rate": 0.0001342666666666667,
      "loss": 2.121,
      "step": 1480
    },
    {
      "epoch": 0.9939131159843242,
      "grad_norm": 0.8262578248977661,
      "learning_rate": 0.00013382222222222223,
      "loss": 2.026,
      "step": 1490
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.3195597529411316,
      "learning_rate": 0.0001333777777777778,
      "loss": 1.8904,
      "step": 1500
    },
    {
      "epoch": 1.006670557825398,
      "grad_norm": 0.8014599680900574,
      "learning_rate": 0.00013293333333333333,
      "loss": 2.0485,
      "step": 1510
    },
    {
      "epoch": 1.0133411156507963,
      "grad_norm": 0.9133009314537048,
      "learning_rate": 0.0001324888888888889,
      "loss": 2.0274,
      "step": 1520
    },
    {
      "epoch": 1.0200116734761944,
      "grad_norm": 1.06565260887146,
      "learning_rate": 0.00013204444444444446,
      "loss": 2.0491,
      "step": 1530
    },
    {
      "epoch": 1.0266822313015926,
      "grad_norm": 0.7244157195091248,
      "learning_rate": 0.0001316,
      "loss": 2.1061,
      "step": 1540
    },
    {
      "epoch": 1.0333527891269907,
      "grad_norm": 0.7507572174072266,
      "learning_rate": 0.00013115555555555557,
      "loss": 2.0385,
      "step": 1550
    },
    {
      "epoch": 1.040023346952389,
      "grad_norm": 0.886350154876709,
      "learning_rate": 0.0001307111111111111,
      "loss": 2.0676,
      "step": 1560
    },
    {
      "epoch": 1.046693904777787,
      "grad_norm": 0.7289033532142639,
      "learning_rate": 0.00013026666666666667,
      "loss": 2.0591,
      "step": 1570
    },
    {
      "epoch": 1.0533644626031853,
      "grad_norm": 0.6557660102844238,
      "learning_rate": 0.0001298222222222222,
      "loss": 2.0839,
      "step": 1580
    },
    {
      "epoch": 1.0600350204285833,
      "grad_norm": 0.7183343768119812,
      "learning_rate": 0.00012937777777777778,
      "loss": 2.0643,
      "step": 1590
    },
    {
      "epoch": 1.0667055782539814,
      "grad_norm": 0.9979502558708191,
      "learning_rate": 0.00012893333333333334,
      "loss": 2.1133,
      "step": 1600
    },
    {
      "epoch": 1.0733761360793797,
      "grad_norm": 0.7240121960639954,
      "learning_rate": 0.0001284888888888889,
      "loss": 2.0439,
      "step": 1610
    },
    {
      "epoch": 1.0800466939047777,
      "grad_norm": 0.9253197312355042,
      "learning_rate": 0.00012804444444444445,
      "loss": 2.0848,
      "step": 1620
    },
    {
      "epoch": 1.086717251730176,
      "grad_norm": 0.7392793297767639,
      "learning_rate": 0.0001276,
      "loss": 2.0966,
      "step": 1630
    },
    {
      "epoch": 1.093387809555574,
      "grad_norm": 0.6076051592826843,
      "learning_rate": 0.00012715555555555555,
      "loss": 2.0688,
      "step": 1640
    },
    {
      "epoch": 1.1000583673809723,
      "grad_norm": 0.6494829654693604,
      "learning_rate": 0.00012671111111111112,
      "loss": 2.1126,
      "step": 1650
    },
    {
      "epoch": 1.1067289252063703,
      "grad_norm": 0.8382177352905273,
      "learning_rate": 0.00012626666666666665,
      "loss": 2.0065,
      "step": 1660
    },
    {
      "epoch": 1.1133994830317686,
      "grad_norm": 0.7481157183647156,
      "learning_rate": 0.00012582222222222222,
      "loss": 2.0497,
      "step": 1670
    },
    {
      "epoch": 1.1200700408571667,
      "grad_norm": 0.9320265650749207,
      "learning_rate": 0.0001253777777777778,
      "loss": 2.0782,
      "step": 1680
    },
    {
      "epoch": 1.1267405986825647,
      "grad_norm": 0.6588038802146912,
      "learning_rate": 0.00012493333333333335,
      "loss": 2.0673,
      "step": 1690
    },
    {
      "epoch": 1.133411156507963,
      "grad_norm": 0.8220767974853516,
      "learning_rate": 0.0001244888888888889,
      "loss": 2.2113,
      "step": 1700
    },
    {
      "epoch": 1.1400817143333613,
      "grad_norm": 0.6817033290863037,
      "learning_rate": 0.00012404444444444446,
      "loss": 1.9918,
      "step": 1710
    },
    {
      "epoch": 1.1467522721587593,
      "grad_norm": 0.8726997375488281,
      "learning_rate": 0.0001236,
      "loss": 2.137,
      "step": 1720
    },
    {
      "epoch": 1.1534228299841573,
      "grad_norm": 0.7142046689987183,
      "learning_rate": 0.00012315555555555556,
      "loss": 2.0187,
      "step": 1730
    },
    {
      "epoch": 1.1600933878095556,
      "grad_norm": 0.8223901391029358,
      "learning_rate": 0.0001227111111111111,
      "loss": 2.1366,
      "step": 1740
    },
    {
      "epoch": 1.1667639456349537,
      "grad_norm": 0.8954200148582458,
      "learning_rate": 0.00012226666666666667,
      "loss": 2.0718,
      "step": 1750
    },
    {
      "epoch": 1.173434503460352,
      "grad_norm": 0.8246024250984192,
      "learning_rate": 0.00012182222222222223,
      "loss": 1.9211,
      "step": 1760
    },
    {
      "epoch": 1.18010506128575,
      "grad_norm": 0.7827885746955872,
      "learning_rate": 0.00012137777777777778,
      "loss": 2.1164,
      "step": 1770
    },
    {
      "epoch": 1.1867756191111483,
      "grad_norm": 0.7649771571159363,
      "learning_rate": 0.00012093333333333334,
      "loss": 2.0048,
      "step": 1780
    },
    {
      "epoch": 1.1934461769365463,
      "grad_norm": 0.8963306546211243,
      "learning_rate": 0.0001204888888888889,
      "loss": 1.9512,
      "step": 1790
    },
    {
      "epoch": 1.2001167347619446,
      "grad_norm": 0.7429786324501038,
      "learning_rate": 0.00012004444444444445,
      "loss": 2.0083,
      "step": 1800
    },
    {
      "epoch": 1.2067872925873426,
      "grad_norm": 0.7340352535247803,
      "learning_rate": 0.00011960000000000001,
      "loss": 2.0099,
      "step": 1810
    },
    {
      "epoch": 1.2134578504127407,
      "grad_norm": 0.831253170967102,
      "learning_rate": 0.00011915555555555556,
      "loss": 2.1874,
      "step": 1820
    },
    {
      "epoch": 1.220128408238139,
      "grad_norm": 0.7196162343025208,
      "learning_rate": 0.00011871111111111111,
      "loss": 1.9993,
      "step": 1830
    },
    {
      "epoch": 1.226798966063537,
      "grad_norm": 0.9584742784500122,
      "learning_rate": 0.00011826666666666668,
      "loss": 2.1447,
      "step": 1840
    },
    {
      "epoch": 1.2334695238889353,
      "grad_norm": 0.8300819396972656,
      "learning_rate": 0.00011782222222222223,
      "loss": 2.0647,
      "step": 1850
    },
    {
      "epoch": 1.2401400817143333,
      "grad_norm": 0.85616534948349,
      "learning_rate": 0.00011737777777777778,
      "loss": 2.0484,
      "step": 1860
    },
    {
      "epoch": 1.2468106395397316,
      "grad_norm": 1.2137609720230103,
      "learning_rate": 0.00011693333333333333,
      "loss": 2.0488,
      "step": 1870
    },
    {
      "epoch": 1.2534811973651296,
      "grad_norm": 0.8354009985923767,
      "learning_rate": 0.00011648888888888889,
      "loss": 2.1409,
      "step": 1880
    },
    {
      "epoch": 1.260151755190528,
      "grad_norm": 0.6995424032211304,
      "learning_rate": 0.00011604444444444444,
      "loss": 2.0426,
      "step": 1890
    },
    {
      "epoch": 1.266822313015926,
      "grad_norm": 0.8455408811569214,
      "learning_rate": 0.00011559999999999999,
      "loss": 1.9953,
      "step": 1900
    },
    {
      "epoch": 1.273492870841324,
      "grad_norm": 0.8432338833808899,
      "learning_rate": 0.00011515555555555557,
      "loss": 2.0377,
      "step": 1910
    },
    {
      "epoch": 1.2801634286667223,
      "grad_norm": 1.026289701461792,
      "learning_rate": 0.00011471111111111112,
      "loss": 1.9948,
      "step": 1920
    },
    {
      "epoch": 1.2868339864921203,
      "grad_norm": 0.6476088762283325,
      "learning_rate": 0.00011426666666666667,
      "loss": 2.0172,
      "step": 1930
    },
    {
      "epoch": 1.2935045443175186,
      "grad_norm": 0.7054911851882935,
      "learning_rate": 0.00011382222222222223,
      "loss": 2.0812,
      "step": 1940
    },
    {
      "epoch": 1.3001751021429166,
      "grad_norm": 0.7581471800804138,
      "learning_rate": 0.00011337777777777778,
      "loss": 2.0952,
      "step": 1950
    },
    {
      "epoch": 1.306845659968315,
      "grad_norm": 0.9613608717918396,
      "learning_rate": 0.00011293333333333333,
      "loss": 2.0905,
      "step": 1960
    },
    {
      "epoch": 1.313516217793713,
      "grad_norm": 0.6181183457374573,
      "learning_rate": 0.00011248888888888888,
      "loss": 2.0167,
      "step": 1970
    },
    {
      "epoch": 1.3201867756191112,
      "grad_norm": 0.8827232122421265,
      "learning_rate": 0.00011204444444444444,
      "loss": 2.1347,
      "step": 1980
    },
    {
      "epoch": 1.3268573334445093,
      "grad_norm": 0.7808615565299988,
      "learning_rate": 0.00011160000000000002,
      "loss": 2.0188,
      "step": 1990
    },
    {
      "epoch": 1.3335278912699073,
      "grad_norm": 0.836570143699646,
      "learning_rate": 0.00011115555555555557,
      "loss": 2.002,
      "step": 2000
    },
    {
      "epoch": 1.3401984490953056,
      "grad_norm": 0.6865046620368958,
      "learning_rate": 0.00011071111111111112,
      "loss": 2.0622,
      "step": 2010
    },
    {
      "epoch": 1.3468690069207039,
      "grad_norm": 0.8346712589263916,
      "learning_rate": 0.00011026666666666667,
      "loss": 1.9594,
      "step": 2020
    },
    {
      "epoch": 1.353539564746102,
      "grad_norm": 0.6335987448692322,
      "learning_rate": 0.00010982222222222222,
      "loss": 2.0933,
      "step": 2030
    },
    {
      "epoch": 1.3602101225715,
      "grad_norm": 0.7003181576728821,
      "learning_rate": 0.00010937777777777778,
      "loss": 2.0279,
      "step": 2040
    },
    {
      "epoch": 1.3668806803968983,
      "grad_norm": 0.7816047668457031,
      "learning_rate": 0.00010893333333333333,
      "loss": 2.1687,
      "step": 2050
    },
    {
      "epoch": 1.3735512382222963,
      "grad_norm": 0.7524048089981079,
      "learning_rate": 0.00010848888888888888,
      "loss": 2.0652,
      "step": 2060
    },
    {
      "epoch": 1.3802217960476946,
      "grad_norm": 0.8389192223548889,
      "learning_rate": 0.00010804444444444446,
      "loss": 2.0294,
      "step": 2070
    },
    {
      "epoch": 1.3868923538730926,
      "grad_norm": 0.8383828997612,
      "learning_rate": 0.00010760000000000001,
      "loss": 2.0646,
      "step": 2080
    },
    {
      "epoch": 1.3935629116984907,
      "grad_norm": 0.993476152420044,
      "learning_rate": 0.00010715555555555557,
      "loss": 2.0324,
      "step": 2090
    },
    {
      "epoch": 1.400233469523889,
      "grad_norm": 0.9132637977600098,
      "learning_rate": 0.00010671111111111112,
      "loss": 2.0412,
      "step": 2100
    },
    {
      "epoch": 1.4069040273492872,
      "grad_norm": 0.9268657565116882,
      "learning_rate": 0.00010626666666666667,
      "loss": 1.9621,
      "step": 2110
    },
    {
      "epoch": 1.4135745851746853,
      "grad_norm": 0.7584657669067383,
      "learning_rate": 0.00010582222222222222,
      "loss": 2.0116,
      "step": 2120
    },
    {
      "epoch": 1.4202451430000833,
      "grad_norm": 0.8596968054771423,
      "learning_rate": 0.00010537777777777777,
      "loss": 2.0649,
      "step": 2130
    },
    {
      "epoch": 1.4269157008254816,
      "grad_norm": 0.7303183078765869,
      "learning_rate": 0.00010493333333333333,
      "loss": 1.9946,
      "step": 2140
    },
    {
      "epoch": 1.4335862586508796,
      "grad_norm": 0.6537812352180481,
      "learning_rate": 0.0001044888888888889,
      "loss": 2.0674,
      "step": 2150
    },
    {
      "epoch": 1.440256816476278,
      "grad_norm": 0.778461217880249,
      "learning_rate": 0.00010404444444444446,
      "loss": 1.9911,
      "step": 2160
    },
    {
      "epoch": 1.446927374301676,
      "grad_norm": 0.8996400833129883,
      "learning_rate": 0.00010360000000000001,
      "loss": 1.9326,
      "step": 2170
    },
    {
      "epoch": 1.453597932127074,
      "grad_norm": 0.8870283961296082,
      "learning_rate": 0.00010315555555555556,
      "loss": 2.0397,
      "step": 2180
    },
    {
      "epoch": 1.4602684899524723,
      "grad_norm": 0.9053886532783508,
      "learning_rate": 0.00010271111111111112,
      "loss": 2.1013,
      "step": 2190
    },
    {
      "epoch": 1.4669390477778705,
      "grad_norm": 0.7321413159370422,
      "learning_rate": 0.00010226666666666667,
      "loss": 2.0425,
      "step": 2200
    },
    {
      "epoch": 1.4736096056032686,
      "grad_norm": 0.7851094603538513,
      "learning_rate": 0.00010182222222222222,
      "loss": 2.046,
      "step": 2210
    },
    {
      "epoch": 1.4802801634286666,
      "grad_norm": 0.9872561693191528,
      "learning_rate": 0.0001013777777777778,
      "loss": 2.0455,
      "step": 2220
    },
    {
      "epoch": 1.486950721254065,
      "grad_norm": 0.9699795246124268,
      "learning_rate": 0.00010093333333333335,
      "loss": 2.0147,
      "step": 2230
    },
    {
      "epoch": 1.493621279079463,
      "grad_norm": 0.6005076169967651,
      "learning_rate": 0.0001004888888888889,
      "loss": 2.0711,
      "step": 2240
    },
    {
      "epoch": 1.5002918369048612,
      "grad_norm": 0.8319035768508911,
      "learning_rate": 0.00010004444444444446,
      "loss": 2.0136,
      "step": 2250
    },
    {
      "epoch": 1.5069623947302593,
      "grad_norm": 0.8447275757789612,
      "learning_rate": 9.960000000000001e-05,
      "loss": 1.9882,
      "step": 2260
    },
    {
      "epoch": 1.5136329525556573,
      "grad_norm": 0.9424229264259338,
      "learning_rate": 9.915555555555556e-05,
      "loss": 2.0212,
      "step": 2270
    },
    {
      "epoch": 1.5203035103810556,
      "grad_norm": 0.8869292140007019,
      "learning_rate": 9.871111111111113e-05,
      "loss": 2.0121,
      "step": 2280
    },
    {
      "epoch": 1.5269740682064539,
      "grad_norm": 0.9384328126907349,
      "learning_rate": 9.826666666666668e-05,
      "loss": 1.9512,
      "step": 2290
    },
    {
      "epoch": 1.533644626031852,
      "grad_norm": 0.811930239200592,
      "learning_rate": 9.782222222222223e-05,
      "loss": 2.0018,
      "step": 2300
    },
    {
      "epoch": 1.54031518385725,
      "grad_norm": 0.8009604811668396,
      "learning_rate": 9.737777777777778e-05,
      "loss": 2.0369,
      "step": 2310
    },
    {
      "epoch": 1.5469857416826482,
      "grad_norm": 0.7113427519798279,
      "learning_rate": 9.693333333333335e-05,
      "loss": 2.0104,
      "step": 2320
    },
    {
      "epoch": 1.5536562995080465,
      "grad_norm": 0.735420823097229,
      "learning_rate": 9.64888888888889e-05,
      "loss": 1.972,
      "step": 2330
    },
    {
      "epoch": 1.5603268573334446,
      "grad_norm": 0.8736763596534729,
      "learning_rate": 9.604444444444445e-05,
      "loss": 2.0197,
      "step": 2340
    },
    {
      "epoch": 1.5669974151588426,
      "grad_norm": 0.8170487284660339,
      "learning_rate": 9.56e-05,
      "loss": 2.0742,
      "step": 2350
    },
    {
      "epoch": 1.5736679729842407,
      "grad_norm": 0.880292534828186,
      "learning_rate": 9.515555555555556e-05,
      "loss": 2.1191,
      "step": 2360
    },
    {
      "epoch": 1.580338530809639,
      "grad_norm": 0.7096930146217346,
      "learning_rate": 9.471111111111111e-05,
      "loss": 1.9698,
      "step": 2370
    },
    {
      "epoch": 1.5870090886350372,
      "grad_norm": 0.9676350355148315,
      "learning_rate": 9.426666666666666e-05,
      "loss": 2.0602,
      "step": 2380
    },
    {
      "epoch": 1.5936796464604353,
      "grad_norm": 0.8021432757377625,
      "learning_rate": 9.382222222222223e-05,
      "loss": 2.0326,
      "step": 2390
    },
    {
      "epoch": 1.6003502042858333,
      "grad_norm": 0.8417640924453735,
      "learning_rate": 9.337777777777778e-05,
      "loss": 2.0612,
      "step": 2400
    },
    {
      "epoch": 1.6070207621112316,
      "grad_norm": 0.9413590431213379,
      "learning_rate": 9.293333333333333e-05,
      "loss": 1.9978,
      "step": 2410
    },
    {
      "epoch": 1.6136913199366298,
      "grad_norm": 0.8380507826805115,
      "learning_rate": 9.248888888888889e-05,
      "loss": 2.0536,
      "step": 2420
    },
    {
      "epoch": 1.620361877762028,
      "grad_norm": 0.7182183861732483,
      "learning_rate": 9.204444444444444e-05,
      "loss": 1.9979,
      "step": 2430
    },
    {
      "epoch": 1.627032435587426,
      "grad_norm": 0.9096441268920898,
      "learning_rate": 9.16e-05,
      "loss": 2.0887,
      "step": 2440
    },
    {
      "epoch": 1.633702993412824,
      "grad_norm": 0.7839715480804443,
      "learning_rate": 9.115555555555556e-05,
      "loss": 2.0437,
      "step": 2450
    },
    {
      "epoch": 1.6403735512382223,
      "grad_norm": 0.8297438025474548,
      "learning_rate": 9.071111111111111e-05,
      "loss": 2.0599,
      "step": 2460
    },
    {
      "epoch": 1.6470441090636205,
      "grad_norm": 1.0476330518722534,
      "learning_rate": 9.026666666666666e-05,
      "loss": 1.9252,
      "step": 2470
    },
    {
      "epoch": 1.6537146668890186,
      "grad_norm": 0.8287738561630249,
      "learning_rate": 8.982222222222223e-05,
      "loss": 1.9409,
      "step": 2480
    },
    {
      "epoch": 1.6603852247144166,
      "grad_norm": 1.0889643430709839,
      "learning_rate": 8.937777777777778e-05,
      "loss": 2.0092,
      "step": 2490
    },
    {
      "epoch": 1.667055782539815,
      "grad_norm": 0.7843400239944458,
      "learning_rate": 8.893333333333333e-05,
      "loss": 1.9597,
      "step": 2500
    },
    {
      "epoch": 1.6737263403652132,
      "grad_norm": 0.6761277914047241,
      "learning_rate": 8.848888888888888e-05,
      "loss": 2.074,
      "step": 2510
    },
    {
      "epoch": 1.6803968981906112,
      "grad_norm": 0.6842317581176758,
      "learning_rate": 8.804444444444445e-05,
      "loss": 2.046,
      "step": 2520
    },
    {
      "epoch": 1.6870674560160093,
      "grad_norm": 0.6146470308303833,
      "learning_rate": 8.76e-05,
      "loss": 2.0525,
      "step": 2530
    },
    {
      "epoch": 1.6937380138414075,
      "grad_norm": 1.0257322788238525,
      "learning_rate": 8.715555555555555e-05,
      "loss": 2.0626,
      "step": 2540
    },
    {
      "epoch": 1.7004085716668056,
      "grad_norm": 0.7646312713623047,
      "learning_rate": 8.671111111111112e-05,
      "loss": 2.0798,
      "step": 2550
    },
    {
      "epoch": 1.7070791294922039,
      "grad_norm": 0.8541821837425232,
      "learning_rate": 8.626666666666667e-05,
      "loss": 2.0926,
      "step": 2560
    },
    {
      "epoch": 1.713749687317602,
      "grad_norm": 0.7120835185050964,
      "learning_rate": 8.582222222222222e-05,
      "loss": 2.0,
      "step": 2570
    },
    {
      "epoch": 1.720420245143,
      "grad_norm": 0.8571082353591919,
      "learning_rate": 8.537777777777778e-05,
      "loss": 1.927,
      "step": 2580
    },
    {
      "epoch": 1.7270908029683982,
      "grad_norm": 0.7695123553276062,
      "learning_rate": 8.493333333333334e-05,
      "loss": 1.9632,
      "step": 2590
    },
    {
      "epoch": 1.7337613607937965,
      "grad_norm": 0.9844435453414917,
      "learning_rate": 8.44888888888889e-05,
      "loss": 2.0577,
      "step": 2600
    },
    {
      "epoch": 1.7404319186191946,
      "grad_norm": 0.9585487842559814,
      "learning_rate": 8.404444444444445e-05,
      "loss": 1.9241,
      "step": 2610
    },
    {
      "epoch": 1.7471024764445926,
      "grad_norm": 0.7298504710197449,
      "learning_rate": 8.36e-05,
      "loss": 2.0984,
      "step": 2620
    },
    {
      "epoch": 1.7537730342699909,
      "grad_norm": 0.975685179233551,
      "learning_rate": 8.315555555555557e-05,
      "loss": 2.0065,
      "step": 2630
    },
    {
      "epoch": 1.7604435920953891,
      "grad_norm": 0.7533746957778931,
      "learning_rate": 8.271111111111112e-05,
      "loss": 2.0393,
      "step": 2640
    },
    {
      "epoch": 1.7671141499207872,
      "grad_norm": 1.012758493423462,
      "learning_rate": 8.226666666666667e-05,
      "loss": 1.9978,
      "step": 2650
    },
    {
      "epoch": 1.7737847077461852,
      "grad_norm": 1.0284298658370972,
      "learning_rate": 8.182222222222222e-05,
      "loss": 2.091,
      "step": 2660
    },
    {
      "epoch": 1.7804552655715833,
      "grad_norm": 0.6846252679824829,
      "learning_rate": 8.137777777777779e-05,
      "loss": 1.9832,
      "step": 2670
    },
    {
      "epoch": 1.7871258233969816,
      "grad_norm": 0.7993494868278503,
      "learning_rate": 8.093333333333334e-05,
      "loss": 2.0392,
      "step": 2680
    },
    {
      "epoch": 1.7937963812223798,
      "grad_norm": 0.79753178358078,
      "learning_rate": 8.048888888888889e-05,
      "loss": 2.008,
      "step": 2690
    },
    {
      "epoch": 1.8004669390477779,
      "grad_norm": 0.8229730129241943,
      "learning_rate": 8.004444444444444e-05,
      "loss": 2.002,
      "step": 2700
    },
    {
      "epoch": 1.807137496873176,
      "grad_norm": 0.8315802216529846,
      "learning_rate": 7.960000000000001e-05,
      "loss": 2.0165,
      "step": 2710
    },
    {
      "epoch": 1.8138080546985742,
      "grad_norm": 0.8947667479515076,
      "learning_rate": 7.915555555555556e-05,
      "loss": 1.9857,
      "step": 2720
    },
    {
      "epoch": 1.8204786125239725,
      "grad_norm": 0.8585730791091919,
      "learning_rate": 7.871111111111111e-05,
      "loss": 1.9243,
      "step": 2730
    },
    {
      "epoch": 1.8271491703493705,
      "grad_norm": 0.7557681798934937,
      "learning_rate": 7.826666666666667e-05,
      "loss": 1.9669,
      "step": 2740
    },
    {
      "epoch": 1.8338197281747686,
      "grad_norm": 0.9212499260902405,
      "learning_rate": 7.782222222222223e-05,
      "loss": 1.9851,
      "step": 2750
    },
    {
      "epoch": 1.8404902860001666,
      "grad_norm": 0.6098167300224304,
      "learning_rate": 7.737777777777779e-05,
      "loss": 2.0616,
      "step": 2760
    },
    {
      "epoch": 1.847160843825565,
      "grad_norm": 0.9238259196281433,
      "learning_rate": 7.693333333333334e-05,
      "loss": 1.9494,
      "step": 2770
    },
    {
      "epoch": 1.8538314016509632,
      "grad_norm": 0.6730668544769287,
      "learning_rate": 7.648888888888889e-05,
      "loss": 2.0174,
      "step": 2780
    },
    {
      "epoch": 1.8605019594763612,
      "grad_norm": 0.8623379468917847,
      "learning_rate": 7.604444444444446e-05,
      "loss": 2.0547,
      "step": 2790
    },
    {
      "epoch": 1.8671725173017593,
      "grad_norm": 0.7153152227401733,
      "learning_rate": 7.560000000000001e-05,
      "loss": 2.0826,
      "step": 2800
    },
    {
      "epoch": 1.8738430751271575,
      "grad_norm": 0.8996075987815857,
      "learning_rate": 7.515555555555556e-05,
      "loss": 2.0666,
      "step": 2810
    },
    {
      "epoch": 1.8805136329525558,
      "grad_norm": 0.9141961932182312,
      "learning_rate": 7.471111111111111e-05,
      "loss": 2.0729,
      "step": 2820
    },
    {
      "epoch": 1.8871841907779539,
      "grad_norm": 0.8660701513290405,
      "learning_rate": 7.426666666666668e-05,
      "loss": 2.0264,
      "step": 2830
    },
    {
      "epoch": 1.893854748603352,
      "grad_norm": 0.8760699033737183,
      "learning_rate": 7.382222222222223e-05,
      "loss": 1.9757,
      "step": 2840
    },
    {
      "epoch": 1.90052530642875,
      "grad_norm": 1.2672325372695923,
      "learning_rate": 7.337777777777778e-05,
      "loss": 2.0636,
      "step": 2850
    },
    {
      "epoch": 1.9071958642541482,
      "grad_norm": 0.7231485843658447,
      "learning_rate": 7.293333333333334e-05,
      "loss": 2.0386,
      "step": 2860
    },
    {
      "epoch": 1.9138664220795465,
      "grad_norm": 0.729621410369873,
      "learning_rate": 7.24888888888889e-05,
      "loss": 2.0503,
      "step": 2870
    },
    {
      "epoch": 1.9205369799049445,
      "grad_norm": 0.8421698808670044,
      "learning_rate": 7.204444444444445e-05,
      "loss": 1.9637,
      "step": 2880
    },
    {
      "epoch": 1.9272075377303426,
      "grad_norm": 0.814813494682312,
      "learning_rate": 7.16e-05,
      "loss": 1.9857,
      "step": 2890
    },
    {
      "epoch": 1.9338780955557409,
      "grad_norm": 0.7560012936592102,
      "learning_rate": 7.115555555555556e-05,
      "loss": 2.0156,
      "step": 2900
    },
    {
      "epoch": 1.9405486533811391,
      "grad_norm": 0.8465632200241089,
      "learning_rate": 7.071111111111111e-05,
      "loss": 1.9848,
      "step": 2910
    },
    {
      "epoch": 1.9472192112065372,
      "grad_norm": 0.6826177835464478,
      "learning_rate": 7.026666666666668e-05,
      "loss": 2.0265,
      "step": 2920
    },
    {
      "epoch": 1.9538897690319352,
      "grad_norm": 0.8898241519927979,
      "learning_rate": 6.982222222222223e-05,
      "loss": 1.9421,
      "step": 2930
    },
    {
      "epoch": 1.9605603268573333,
      "grad_norm": 0.7259095311164856,
      "learning_rate": 6.937777777777778e-05,
      "loss": 2.0554,
      "step": 2940
    },
    {
      "epoch": 1.9672308846827316,
      "grad_norm": 0.6880371570587158,
      "learning_rate": 6.893333333333333e-05,
      "loss": 2.1218,
      "step": 2950
    },
    {
      "epoch": 1.9739014425081298,
      "grad_norm": 0.8842959403991699,
      "learning_rate": 6.848888888888889e-05,
      "loss": 1.9169,
      "step": 2960
    },
    {
      "epoch": 1.9805720003335279,
      "grad_norm": 0.8997195363044739,
      "learning_rate": 6.804444444444444e-05,
      "loss": 2.0166,
      "step": 2970
    },
    {
      "epoch": 1.987242558158926,
      "grad_norm": 0.8367184996604919,
      "learning_rate": 6.76e-05,
      "loss": 2.0091,
      "step": 2980
    },
    {
      "epoch": 1.9939131159843242,
      "grad_norm": 0.6979811787605286,
      "learning_rate": 6.715555555555556e-05,
      "loss": 2.059,
      "step": 2990
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.4937320053577423,
      "learning_rate": 6.671111111111111e-05,
      "loss": 1.7716,
      "step": 3000
    },
    {
      "epoch": 2.006670557825398,
      "grad_norm": 0.9744850397109985,
      "learning_rate": 6.626666666666666e-05,
      "loss": 1.9895,
      "step": 3010
    },
    {
      "epoch": 2.013341115650796,
      "grad_norm": 0.6749591827392578,
      "learning_rate": 6.582222222222223e-05,
      "loss": 1.9293,
      "step": 3020
    },
    {
      "epoch": 2.0200116734761946,
      "grad_norm": 0.7772794961929321,
      "learning_rate": 6.537777777777778e-05,
      "loss": 1.9982,
      "step": 3030
    },
    {
      "epoch": 2.0266822313015926,
      "grad_norm": 0.9178017973899841,
      "learning_rate": 6.493333333333333e-05,
      "loss": 2.0341,
      "step": 3040
    },
    {
      "epoch": 2.0333527891269907,
      "grad_norm": 0.8069935441017151,
      "learning_rate": 6.448888888888888e-05,
      "loss": 1.9972,
      "step": 3050
    },
    {
      "epoch": 2.0400233469523887,
      "grad_norm": 0.8349827527999878,
      "learning_rate": 6.404444444444445e-05,
      "loss": 2.0534,
      "step": 3060
    },
    {
      "epoch": 2.0466939047777872,
      "grad_norm": 0.8576868176460266,
      "learning_rate": 6.36e-05,
      "loss": 2.0463,
      "step": 3070
    },
    {
      "epoch": 2.0533644626031853,
      "grad_norm": 0.8253921866416931,
      "learning_rate": 6.315555555555555e-05,
      "loss": 1.9948,
      "step": 3080
    },
    {
      "epoch": 2.0600350204285833,
      "grad_norm": 0.7563795447349548,
      "learning_rate": 6.27111111111111e-05,
      "loss": 1.994,
      "step": 3090
    },
    {
      "epoch": 2.0667055782539814,
      "grad_norm": 0.7768071889877319,
      "learning_rate": 6.226666666666667e-05,
      "loss": 2.0137,
      "step": 3100
    },
    {
      "epoch": 2.0733761360793794,
      "grad_norm": 1.0642958879470825,
      "learning_rate": 6.182222222222222e-05,
      "loss": 1.95,
      "step": 3110
    },
    {
      "epoch": 2.080046693904778,
      "grad_norm": 0.9651339650154114,
      "learning_rate": 6.137777777777778e-05,
      "loss": 1.9746,
      "step": 3120
    },
    {
      "epoch": 2.086717251730176,
      "grad_norm": 0.956343412399292,
      "learning_rate": 6.093333333333333e-05,
      "loss": 2.0152,
      "step": 3130
    },
    {
      "epoch": 2.093387809555574,
      "grad_norm": 0.8009855151176453,
      "learning_rate": 6.0488888888888894e-05,
      "loss": 1.9489,
      "step": 3140
    },
    {
      "epoch": 2.100058367380972,
      "grad_norm": 0.835134744644165,
      "learning_rate": 6.0044444444444446e-05,
      "loss": 1.9169,
      "step": 3150
    },
    {
      "epoch": 2.1067289252063706,
      "grad_norm": 0.7410443425178528,
      "learning_rate": 5.96e-05,
      "loss": 2.0505,
      "step": 3160
    },
    {
      "epoch": 2.1133994830317686,
      "grad_norm": 0.8946512937545776,
      "learning_rate": 5.915555555555555e-05,
      "loss": 2.019,
      "step": 3170
    },
    {
      "epoch": 2.1200700408571667,
      "grad_norm": 0.7533782124519348,
      "learning_rate": 5.871111111111112e-05,
      "loss": 2.0822,
      "step": 3180
    },
    {
      "epoch": 2.1267405986825647,
      "grad_norm": 0.8390100002288818,
      "learning_rate": 5.826666666666667e-05,
      "loss": 1.9515,
      "step": 3190
    },
    {
      "epoch": 2.1334111565079628,
      "grad_norm": 1.057142972946167,
      "learning_rate": 5.782222222222222e-05,
      "loss": 2.0179,
      "step": 3200
    },
    {
      "epoch": 2.1400817143333613,
      "grad_norm": 1.0296952724456787,
      "learning_rate": 5.737777777777779e-05,
      "loss": 2.0523,
      "step": 3210
    },
    {
      "epoch": 2.1467522721587593,
      "grad_norm": 1.0260276794433594,
      "learning_rate": 5.693333333333334e-05,
      "loss": 1.9629,
      "step": 3220
    },
    {
      "epoch": 2.1534228299841573,
      "grad_norm": 0.737949013710022,
      "learning_rate": 5.648888888888889e-05,
      "loss": 1.9417,
      "step": 3230
    },
    {
      "epoch": 2.1600933878095554,
      "grad_norm": 0.8765841722488403,
      "learning_rate": 5.6044444444444444e-05,
      "loss": 2.0324,
      "step": 3240
    },
    {
      "epoch": 2.166763945634954,
      "grad_norm": 1.0032881498336792,
      "learning_rate": 5.560000000000001e-05,
      "loss": 2.0391,
      "step": 3250
    },
    {
      "epoch": 2.173434503460352,
      "grad_norm": 0.7597514390945435,
      "learning_rate": 5.515555555555556e-05,
      "loss": 2.0164,
      "step": 3260
    },
    {
      "epoch": 2.18010506128575,
      "grad_norm": 0.7491661310195923,
      "learning_rate": 5.4711111111111114e-05,
      "loss": 2.0054,
      "step": 3270
    },
    {
      "epoch": 2.186775619111148,
      "grad_norm": 0.8559471368789673,
      "learning_rate": 5.4266666666666667e-05,
      "loss": 2.0021,
      "step": 3280
    },
    {
      "epoch": 2.193446176936546,
      "grad_norm": 1.1118736267089844,
      "learning_rate": 5.382222222222223e-05,
      "loss": 1.9614,
      "step": 3290
    },
    {
      "epoch": 2.2001167347619446,
      "grad_norm": 0.781133770942688,
      "learning_rate": 5.3377777777777785e-05,
      "loss": 1.9975,
      "step": 3300
    },
    {
      "epoch": 2.2067872925873426,
      "grad_norm": 0.9462118148803711,
      "learning_rate": 5.293333333333334e-05,
      "loss": 2.0395,
      "step": 3310
    },
    {
      "epoch": 2.2134578504127407,
      "grad_norm": 0.7191663980484009,
      "learning_rate": 5.248888888888889e-05,
      "loss": 2.1232,
      "step": 3320
    },
    {
      "epoch": 2.2201284082381387,
      "grad_norm": 0.6939757466316223,
      "learning_rate": 5.204444444444445e-05,
      "loss": 1.9651,
      "step": 3330
    },
    {
      "epoch": 2.226798966063537,
      "grad_norm": 0.8239359259605408,
      "learning_rate": 5.16e-05,
      "loss": 2.0076,
      "step": 3340
    },
    {
      "epoch": 2.2334695238889353,
      "grad_norm": 0.9939097166061401,
      "learning_rate": 5.115555555555556e-05,
      "loss": 2.0461,
      "step": 3350
    },
    {
      "epoch": 2.2401400817143333,
      "grad_norm": 0.7855533361434937,
      "learning_rate": 5.071111111111111e-05,
      "loss": 2.0806,
      "step": 3360
    },
    {
      "epoch": 2.2468106395397314,
      "grad_norm": 0.8440476059913635,
      "learning_rate": 5.026666666666667e-05,
      "loss": 1.9546,
      "step": 3370
    },
    {
      "epoch": 2.2534811973651294,
      "grad_norm": 0.7862440943717957,
      "learning_rate": 4.982222222222222e-05,
      "loss": 2.0316,
      "step": 3380
    },
    {
      "epoch": 2.260151755190528,
      "grad_norm": 0.883792519569397,
      "learning_rate": 4.9377777777777776e-05,
      "loss": 1.95,
      "step": 3390
    },
    {
      "epoch": 2.266822313015926,
      "grad_norm": 0.650589108467102,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 2.0042,
      "step": 3400
    },
    {
      "epoch": 2.273492870841324,
      "grad_norm": 0.8339083194732666,
      "learning_rate": 4.848888888888889e-05,
      "loss": 1.996,
      "step": 3410
    },
    {
      "epoch": 2.2801634286667225,
      "grad_norm": 0.7746338248252869,
      "learning_rate": 4.8044444444444446e-05,
      "loss": 1.9503,
      "step": 3420
    },
    {
      "epoch": 2.2868339864921206,
      "grad_norm": 1.366612434387207,
      "learning_rate": 4.76e-05,
      "loss": 2.0174,
      "step": 3430
    },
    {
      "epoch": 2.2935045443175186,
      "grad_norm": 0.7892959713935852,
      "learning_rate": 4.715555555555556e-05,
      "loss": 1.9763,
      "step": 3440
    },
    {
      "epoch": 2.3001751021429166,
      "grad_norm": 0.8568168878555298,
      "learning_rate": 4.671111111111111e-05,
      "loss": 1.9494,
      "step": 3450
    },
    {
      "epoch": 2.3068456599683147,
      "grad_norm": 1.0460251569747925,
      "learning_rate": 4.626666666666667e-05,
      "loss": 2.0344,
      "step": 3460
    },
    {
      "epoch": 2.313516217793713,
      "grad_norm": 0.8660480976104736,
      "learning_rate": 4.582222222222222e-05,
      "loss": 2.0021,
      "step": 3470
    },
    {
      "epoch": 2.3201867756191112,
      "grad_norm": 0.7216047644615173,
      "learning_rate": 4.537777777777778e-05,
      "loss": 1.9959,
      "step": 3480
    },
    {
      "epoch": 2.3268573334445093,
      "grad_norm": 0.9531814455986023,
      "learning_rate": 4.493333333333333e-05,
      "loss": 2.0685,
      "step": 3490
    },
    {
      "epoch": 2.3335278912699073,
      "grad_norm": 0.792163610458374,
      "learning_rate": 4.448888888888889e-05,
      "loss": 1.9404,
      "step": 3500
    },
    {
      "epoch": 2.340198449095306,
      "grad_norm": 0.8890946507453918,
      "learning_rate": 4.404444444444445e-05,
      "loss": 1.9909,
      "step": 3510
    },
    {
      "epoch": 2.346869006920704,
      "grad_norm": 0.9658938646316528,
      "learning_rate": 4.36e-05,
      "loss": 2.0022,
      "step": 3520
    },
    {
      "epoch": 2.353539564746102,
      "grad_norm": 0.9756982326507568,
      "learning_rate": 4.315555555555556e-05,
      "loss": 1.9811,
      "step": 3530
    },
    {
      "epoch": 2.3602101225715,
      "grad_norm": 0.6686705350875854,
      "learning_rate": 4.2711111111111114e-05,
      "loss": 2.0061,
      "step": 3540
    },
    {
      "epoch": 2.366880680396898,
      "grad_norm": 0.7480409741401672,
      "learning_rate": 4.226666666666667e-05,
      "loss": 2.0593,
      "step": 3550
    },
    {
      "epoch": 2.3735512382222965,
      "grad_norm": 0.8916929960250854,
      "learning_rate": 4.1822222222222225e-05,
      "loss": 1.9545,
      "step": 3560
    },
    {
      "epoch": 2.3802217960476946,
      "grad_norm": 0.9062147736549377,
      "learning_rate": 4.1377777777777784e-05,
      "loss": 2.0009,
      "step": 3570
    },
    {
      "epoch": 2.3868923538730926,
      "grad_norm": 0.8574641346931458,
      "learning_rate": 4.093333333333334e-05,
      "loss": 1.9707,
      "step": 3580
    },
    {
      "epoch": 2.3935629116984907,
      "grad_norm": 0.7335342168807983,
      "learning_rate": 4.0488888888888896e-05,
      "loss": 2.0612,
      "step": 3590
    },
    {
      "epoch": 2.400233469523889,
      "grad_norm": 0.8652471303939819,
      "learning_rate": 4.004444444444445e-05,
      "loss": 2.0596,
      "step": 3600
    },
    {
      "epoch": 2.406904027349287,
      "grad_norm": 0.8741504549980164,
      "learning_rate": 3.960000000000001e-05,
      "loss": 1.9627,
      "step": 3610
    },
    {
      "epoch": 2.4135745851746853,
      "grad_norm": 0.8310292959213257,
      "learning_rate": 3.915555555555556e-05,
      "loss": 2.0473,
      "step": 3620
    },
    {
      "epoch": 2.4202451430000833,
      "grad_norm": 0.9889411330223083,
      "learning_rate": 3.871111111111111e-05,
      "loss": 2.0812,
      "step": 3630
    },
    {
      "epoch": 2.4269157008254814,
      "grad_norm": 0.9964048862457275,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 2.0206,
      "step": 3640
    },
    {
      "epoch": 2.43358625865088,
      "grad_norm": 0.6713144779205322,
      "learning_rate": 3.782222222222222e-05,
      "loss": 2.0328,
      "step": 3650
    },
    {
      "epoch": 2.440256816476278,
      "grad_norm": 0.9575945734977722,
      "learning_rate": 3.7377777777777775e-05,
      "loss": 1.9849,
      "step": 3660
    },
    {
      "epoch": 2.446927374301676,
      "grad_norm": 0.9604154825210571,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 2.0313,
      "step": 3670
    },
    {
      "epoch": 2.453597932127074,
      "grad_norm": 0.8950493931770325,
      "learning_rate": 3.648888888888889e-05,
      "loss": 2.0766,
      "step": 3680
    },
    {
      "epoch": 2.4602684899524725,
      "grad_norm": 0.839576244354248,
      "learning_rate": 3.6044444444444446e-05,
      "loss": 1.9025,
      "step": 3690
    },
    {
      "epoch": 2.4669390477778705,
      "grad_norm": 0.7938646674156189,
      "learning_rate": 3.56e-05,
      "loss": 1.8508,
      "step": 3700
    },
    {
      "epoch": 2.4736096056032686,
      "grad_norm": 1.0421733856201172,
      "learning_rate": 3.515555555555556e-05,
      "loss": 1.9786,
      "step": 3710
    },
    {
      "epoch": 2.4802801634286666,
      "grad_norm": 0.930597186088562,
      "learning_rate": 3.471111111111111e-05,
      "loss": 1.9418,
      "step": 3720
    },
    {
      "epoch": 2.4869507212540647,
      "grad_norm": 0.8345292806625366,
      "learning_rate": 3.426666666666667e-05,
      "loss": 1.9196,
      "step": 3730
    },
    {
      "epoch": 2.493621279079463,
      "grad_norm": 0.8603196740150452,
      "learning_rate": 3.382222222222222e-05,
      "loss": 1.9874,
      "step": 3740
    },
    {
      "epoch": 2.5002918369048612,
      "grad_norm": 0.7679088711738586,
      "learning_rate": 3.337777777777778e-05,
      "loss": 1.9718,
      "step": 3750
    },
    {
      "epoch": 2.5069623947302593,
      "grad_norm": 0.7803390622138977,
      "learning_rate": 3.293333333333333e-05,
      "loss": 2.0117,
      "step": 3760
    },
    {
      "epoch": 2.5136329525556573,
      "grad_norm": 0.9281098246574402,
      "learning_rate": 3.248888888888889e-05,
      "loss": 2.0041,
      "step": 3770
    },
    {
      "epoch": 2.520303510381056,
      "grad_norm": 0.8557931780815125,
      "learning_rate": 3.204444444444444e-05,
      "loss": 2.0042,
      "step": 3780
    },
    {
      "epoch": 2.526974068206454,
      "grad_norm": 0.9168921113014221,
      "learning_rate": 3.16e-05,
      "loss": 1.9172,
      "step": 3790
    },
    {
      "epoch": 2.533644626031852,
      "grad_norm": 0.8787859678268433,
      "learning_rate": 3.1155555555555555e-05,
      "loss": 2.0608,
      "step": 3800
    },
    {
      "epoch": 2.54031518385725,
      "grad_norm": 0.8464169502258301,
      "learning_rate": 3.0711111111111114e-05,
      "loss": 2.1327,
      "step": 3810
    },
    {
      "epoch": 2.546985741682648,
      "grad_norm": 1.0448018312454224,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 2.033,
      "step": 3820
    },
    {
      "epoch": 2.5536562995080465,
      "grad_norm": 0.863330602645874,
      "learning_rate": 2.9822222222222225e-05,
      "loss": 1.9509,
      "step": 3830
    },
    {
      "epoch": 2.5603268573334446,
      "grad_norm": 1.3381292819976807,
      "learning_rate": 2.937777777777778e-05,
      "loss": 1.9014,
      "step": 3840
    },
    {
      "epoch": 2.5669974151588426,
      "grad_norm": 1.017756462097168,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 2.0327,
      "step": 3850
    },
    {
      "epoch": 2.5736679729842407,
      "grad_norm": 0.8226451873779297,
      "learning_rate": 2.8488888888888892e-05,
      "loss": 1.9384,
      "step": 3860
    },
    {
      "epoch": 2.580338530809639,
      "grad_norm": 0.8617264032363892,
      "learning_rate": 2.8044444444444444e-05,
      "loss": 1.9734,
      "step": 3870
    },
    {
      "epoch": 2.587009088635037,
      "grad_norm": 0.8324326276779175,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 2.0461,
      "step": 3880
    },
    {
      "epoch": 2.5936796464604353,
      "grad_norm": 0.890058696269989,
      "learning_rate": 2.7155555555555556e-05,
      "loss": 1.987,
      "step": 3890
    },
    {
      "epoch": 2.6003502042858333,
      "grad_norm": 1.0521653890609741,
      "learning_rate": 2.6711111111111115e-05,
      "loss": 2.0508,
      "step": 3900
    },
    {
      "epoch": 2.6070207621112313,
      "grad_norm": 1.1493933200836182,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 2.0765,
      "step": 3910
    },
    {
      "epoch": 2.61369131993663,
      "grad_norm": 0.9742928743362427,
      "learning_rate": 2.5822222222222226e-05,
      "loss": 2.0615,
      "step": 3920
    },
    {
      "epoch": 2.620361877762028,
      "grad_norm": 1.1214929819107056,
      "learning_rate": 2.537777777777778e-05,
      "loss": 2.0817,
      "step": 3930
    },
    {
      "epoch": 2.627032435587426,
      "grad_norm": 0.6930075883865356,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 1.9714,
      "step": 3940
    },
    {
      "epoch": 2.633702993412824,
      "grad_norm": 0.7739746570587158,
      "learning_rate": 2.448888888888889e-05,
      "loss": 2.0218,
      "step": 3950
    },
    {
      "epoch": 2.6403735512382225,
      "grad_norm": 0.8225243091583252,
      "learning_rate": 2.4044444444444445e-05,
      "loss": 2.0174,
      "step": 3960
    },
    {
      "epoch": 2.6470441090636205,
      "grad_norm": 0.9721664190292358,
      "learning_rate": 2.36e-05,
      "loss": 2.0182,
      "step": 3970
    },
    {
      "epoch": 2.6537146668890186,
      "grad_norm": 0.8286255598068237,
      "learning_rate": 2.3155555555555557e-05,
      "loss": 2.0184,
      "step": 3980
    },
    {
      "epoch": 2.6603852247144166,
      "grad_norm": 0.7847057580947876,
      "learning_rate": 2.2711111111111112e-05,
      "loss": 2.0516,
      "step": 3990
    },
    {
      "epoch": 2.6670557825398147,
      "grad_norm": 0.8277118802070618,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 1.9982,
      "step": 4000
    },
    {
      "epoch": 2.673726340365213,
      "grad_norm": 0.7897570133209229,
      "learning_rate": 2.1822222222222224e-05,
      "loss": 2.0656,
      "step": 4010
    },
    {
      "epoch": 2.680396898190611,
      "grad_norm": 0.8123354911804199,
      "learning_rate": 2.137777777777778e-05,
      "loss": 1.9316,
      "step": 4020
    },
    {
      "epoch": 2.6870674560160093,
      "grad_norm": 0.9297161102294922,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 1.9812,
      "step": 4030
    },
    {
      "epoch": 2.6937380138414078,
      "grad_norm": 0.8102401494979858,
      "learning_rate": 2.048888888888889e-05,
      "loss": 2.0598,
      "step": 4040
    },
    {
      "epoch": 2.700408571666806,
      "grad_norm": 0.761492133140564,
      "learning_rate": 2.0044444444444446e-05,
      "loss": 2.0215,
      "step": 4050
    },
    {
      "epoch": 2.707079129492204,
      "grad_norm": 0.7753345370292664,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 1.9601,
      "step": 4060
    },
    {
      "epoch": 2.713749687317602,
      "grad_norm": 1.0555243492126465,
      "learning_rate": 1.9155555555555558e-05,
      "loss": 2.0365,
      "step": 4070
    },
    {
      "epoch": 2.720420245143,
      "grad_norm": 0.9421618580818176,
      "learning_rate": 1.8711111111111113e-05,
      "loss": 1.9094,
      "step": 4080
    },
    {
      "epoch": 2.727090802968398,
      "grad_norm": 0.7920216917991638,
      "learning_rate": 1.826666666666667e-05,
      "loss": 2.0122,
      "step": 4090
    },
    {
      "epoch": 2.7337613607937965,
      "grad_norm": 0.9718508124351501,
      "learning_rate": 1.7822222222222225e-05,
      "loss": 2.0148,
      "step": 4100
    },
    {
      "epoch": 2.7404319186191946,
      "grad_norm": 0.8967978358268738,
      "learning_rate": 1.737777777777778e-05,
      "loss": 2.0195,
      "step": 4110
    },
    {
      "epoch": 2.7471024764445926,
      "grad_norm": 1.0559916496276855,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 2.0603,
      "step": 4120
    },
    {
      "epoch": 2.753773034269991,
      "grad_norm": 0.8368217349052429,
      "learning_rate": 1.648888888888889e-05,
      "loss": 1.9697,
      "step": 4130
    },
    {
      "epoch": 2.760443592095389,
      "grad_norm": 0.9197049736976624,
      "learning_rate": 1.6044444444444444e-05,
      "loss": 2.0059,
      "step": 4140
    },
    {
      "epoch": 2.767114149920787,
      "grad_norm": 0.9542078375816345,
      "learning_rate": 1.56e-05,
      "loss": 1.9798,
      "step": 4150
    },
    {
      "epoch": 2.7737847077461852,
      "grad_norm": 1.0650413036346436,
      "learning_rate": 1.5155555555555555e-05,
      "loss": 1.9465,
      "step": 4160
    },
    {
      "epoch": 2.7804552655715833,
      "grad_norm": 0.7528480887413025,
      "learning_rate": 1.4711111111111111e-05,
      "loss": 2.0264,
      "step": 4170
    },
    {
      "epoch": 2.7871258233969813,
      "grad_norm": 0.8800252079963684,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 1.9787,
      "step": 4180
    },
    {
      "epoch": 2.79379638122238,
      "grad_norm": 1.1964495182037354,
      "learning_rate": 1.3822222222222222e-05,
      "loss": 1.9715,
      "step": 4190
    },
    {
      "epoch": 2.800466939047778,
      "grad_norm": 0.8062859177589417,
      "learning_rate": 1.3377777777777778e-05,
      "loss": 1.9725,
      "step": 4200
    },
    {
      "epoch": 2.807137496873176,
      "grad_norm": 0.9458860754966736,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 2.0659,
      "step": 4210
    },
    {
      "epoch": 2.8138080546985744,
      "grad_norm": 0.9021795392036438,
      "learning_rate": 1.248888888888889e-05,
      "loss": 1.9985,
      "step": 4220
    },
    {
      "epoch": 2.8204786125239725,
      "grad_norm": 0.7931868433952332,
      "learning_rate": 1.2044444444444445e-05,
      "loss": 1.9548,
      "step": 4230
    },
    {
      "epoch": 2.8271491703493705,
      "grad_norm": 0.7402334809303284,
      "learning_rate": 1.16e-05,
      "loss": 1.9803,
      "step": 4240
    },
    {
      "epoch": 2.8338197281747686,
      "grad_norm": 0.7140700817108154,
      "learning_rate": 1.1155555555555556e-05,
      "loss": 1.9533,
      "step": 4250
    },
    {
      "epoch": 2.8404902860001666,
      "grad_norm": 0.9068136215209961,
      "learning_rate": 1.0711111111111112e-05,
      "loss": 1.9703,
      "step": 4260
    },
    {
      "epoch": 2.8471608438255647,
      "grad_norm": 1.1460988521575928,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 1.9943,
      "step": 4270
    },
    {
      "epoch": 2.853831401650963,
      "grad_norm": 0.8110398650169373,
      "learning_rate": 9.822222222222223e-06,
      "loss": 2.0302,
      "step": 4280
    },
    {
      "epoch": 2.860501959476361,
      "grad_norm": 0.9587580561637878,
      "learning_rate": 9.377777777777779e-06,
      "loss": 1.9455,
      "step": 4290
    },
    {
      "epoch": 2.8671725173017593,
      "grad_norm": 0.7572201490402222,
      "learning_rate": 8.933333333333333e-06,
      "loss": 2.0611,
      "step": 4300
    },
    {
      "epoch": 2.8738430751271578,
      "grad_norm": 0.9282757043838501,
      "learning_rate": 8.488888888888889e-06,
      "loss": 1.9874,
      "step": 4310
    },
    {
      "epoch": 2.880513632952556,
      "grad_norm": 0.8917367458343506,
      "learning_rate": 8.08888888888889e-06,
      "loss": 2.0454,
      "step": 4320
    },
    {
      "epoch": 2.887184190777954,
      "grad_norm": 0.7595371603965759,
      "learning_rate": 7.644444444444445e-06,
      "loss": 2.0283,
      "step": 4330
    },
    {
      "epoch": 2.893854748603352,
      "grad_norm": 0.8255163431167603,
      "learning_rate": 7.2e-06,
      "loss": 1.9638,
      "step": 4340
    },
    {
      "epoch": 2.90052530642875,
      "grad_norm": 0.8291995525360107,
      "learning_rate": 6.755555555555555e-06,
      "loss": 1.9163,
      "step": 4350
    },
    {
      "epoch": 2.907195864254148,
      "grad_norm": 1.229978084564209,
      "learning_rate": 6.311111111111112e-06,
      "loss": 2.0278,
      "step": 4360
    },
    {
      "epoch": 2.9138664220795465,
      "grad_norm": 0.7294237017631531,
      "learning_rate": 5.866666666666667e-06,
      "loss": 1.949,
      "step": 4370
    },
    {
      "epoch": 2.9205369799049445,
      "grad_norm": 0.8973362445831299,
      "learning_rate": 5.422222222222222e-06,
      "loss": 1.9946,
      "step": 4380
    },
    {
      "epoch": 2.9272075377303426,
      "grad_norm": 1.0686379671096802,
      "learning_rate": 4.977777777777778e-06,
      "loss": 1.9937,
      "step": 4390
    },
    {
      "epoch": 2.933878095555741,
      "grad_norm": 0.8441746830940247,
      "learning_rate": 4.533333333333334e-06,
      "loss": 1.9198,
      "step": 4400
    },
    {
      "epoch": 2.940548653381139,
      "grad_norm": 0.7875902652740479,
      "learning_rate": 4.088888888888889e-06,
      "loss": 1.9235,
      "step": 4410
    },
    {
      "epoch": 2.947219211206537,
      "grad_norm": 1.0151013135910034,
      "learning_rate": 3.6444444444444446e-06,
      "loss": 1.9627,
      "step": 4420
    },
    {
      "epoch": 2.9538897690319352,
      "grad_norm": 0.7992688417434692,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 1.9955,
      "step": 4430
    },
    {
      "epoch": 2.9605603268573333,
      "grad_norm": 0.9382109045982361,
      "learning_rate": 2.7555555555555555e-06,
      "loss": 1.8973,
      "step": 4440
    },
    {
      "epoch": 2.9672308846827313,
      "grad_norm": 0.9217323660850525,
      "learning_rate": 2.311111111111111e-06,
      "loss": 1.9352,
      "step": 4450
    },
    {
      "epoch": 2.97390144250813,
      "grad_norm": 0.9186627864837646,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 1.9776,
      "step": 4460
    },
    {
      "epoch": 2.980572000333528,
      "grad_norm": 0.9893046617507935,
      "learning_rate": 1.4222222222222223e-06,
      "loss": 2.0001,
      "step": 4470
    },
    {
      "epoch": 2.987242558158926,
      "grad_norm": 1.0615742206573486,
      "learning_rate": 9.777777777777778e-07,
      "loss": 1.9881,
      "step": 4480
    },
    {
      "epoch": 2.9939131159843244,
      "grad_norm": 0.8216977119445801,
      "learning_rate": 5.333333333333333e-07,
      "loss": 1.9505,
      "step": 4490
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.529044508934021,
      "learning_rate": 8.88888888888889e-08,
      "loss": 1.768,
      "step": 4500
    }
  ],
  "logging_steps": 10,
  "max_steps": 4500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.066463288611635e+16,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
